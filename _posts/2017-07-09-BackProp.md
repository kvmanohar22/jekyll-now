---
layout: post
title: Backpropagation
comments: true
---

<link rel="stylesheet" href="./../js/highlight.js/styles/default.css">
<script src="./../js/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


Contents:

- [Cross Entropy Loss](#cross_entropy)
- [Derivatives of Softmax](#der_soft_act)
- [Network Architecture](#network)
- [Softmax Activation](#soft_act)


Inspired from training Deep Neural Networks up till now, I wanted to write about
the working of backpropagation and the flow of gradients during backpropagation.

Some of the commonly used loss functions in training models are as follows:
- [Absolute Difference Loss](https://en.wikipedia.org/wiki/Absolute_difference)
- [Cosine Distance Loss](https://en.wikipedia.org/wiki/Cosine_similarity)
- [Cross Entropy Loss](https://en.wikipedia.org/wiki/Cross_entropy)
- [Hinge Loss](https://en.wikipedia.org/wiki/Huber_loss)
- [Huber Loss](https://en.wikipedia.org/wiki/Hinge_loss)
- [Mean Squared Loss](https://en.wikipedia.org/wiki/Mean_squared_error)


Let's go through the feed forward and backpropagation for a fully connected network for the task of classification. We shall use **softmax** activation with **Cross Entropy loss** for the final layer.

<a name='soft_act'></a>

## Softmax Activation
This Activation takes in an N-dimensional vector and produces another N-dimensional vector after applying a non-linearity. The actual function \\(f: \mathbb{R}^{N} \rightarrow \mathbb{R}^{N}\\) is as follows:

$$ f_{i}^{} = \frac{e^{p_{i}}}{\sum_{j=1}^{N}e^{p_{j}}} \quad \forall i \in [1, N] $$

Computing softmax activation is as follows:

<pre>
  <code class="cpp">
    def softmax_activation(x):
      """
      Compute the softmax activation of vector `x`
      """
      y = np.exp(x) / np.sum(np.exp(x))
      return y
  </code>
</pre>

Consider $$x = [1.0, 2.0, 3.0]$$
<pre>
  <code class="cpp">
    softmax_activation([1.0, 2.0, 3.0])
    [ 0.09003057,  0.24472847,  0.66524096]
  </code>
</pre>

Now consider, $$x = [1000.0, 2000.0, 3000.0]$$

<pre>
  <code class="cpp">
    softmax_activation([1000.0, 2000.0, 3000.0])
    [ nan,  nan,  nan]
  </code>
</pre>

This occurs due to the overflow which is encountered in `exp`. To ensure that we don't blow up the activation values, we need to normalize the input vector. This is done by subtracting element wise the max element from the input vector \\(x\\). This operation makes all the elements of the vector negative except for the max element which is zero. Thus even for a very large negative element, the softmax on this element returns a value close to zero and we can avoid Nan.

<pre>
  <code class="cpp">
    def stable_softmax_activation(x):
      """
      Compute the stable softmax activation of vector `x`
      """
      y = np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))
      return y
  </code>
</pre>

Now, for $$x = [1000.0, 2000.0, 3000.0]$$

<pre>
  <code class="cpp">
    softmax_activation([1000.0, 2000.0, 3000.0])
    [ 0.,  0.,  1.]
  </code>
</pre>


<a name='der_soft_act'></a>

## Partial Derivatives of Softmax Activation
$$X = [p_{1}, p_{2}, ..., p_{N}]$$ called the "logits" and $$f(X)$$ computes the softmax activations which are:
$$Y = [f_{1}, f_{2}, ..., f_{N}]$$

We need to now compute the Jacobian matrix for the \\(i\\)th output w.r.t \\(j\\)th input:

$$
\frac{\partial f_{i}}{\partial p_{j}} \quad \forall i, j \in [1, N]
$$

$$
\frac{\partial f_{i}}{\partial p_{j}} =  \frac{\partial \frac{e^{p_{i}}}{\sum_{k=1}^{N}e^{p_{k}}}}{\partial p_{j}}
$$

For \\(i = j\\), we have:

$$
\begin{align}
\frac{\partial f_{i}}{\partial p_{j}} & = \frac{\left(\sum_{k=1}^{N}e^{p_{k}} \right) e^{p_{i}} - e^{p_{i}} e^{p_{j}}}{\left(\sum_{k=1}^{N}e^{p_{k}}\right)^{2}} \\
& = \frac{e^{p_{i}}}{\sum_{k=1}^{N}e^{p_{k}}} - \frac{e^{p_{i}}}{\sum_{k=1}^{N}e^{p_{k}}}\frac{e^{p_{j}}}{\sum_{k=1}^{N}e^{p_{k}}} \\[2ex]
& = f_{i} - f_{i} f_{j} \\
& = f_{i}\left(1-f_{j}\right)
\end{align}
$$

And for the case \\(i \neq j\\), we have:

$$
\begin{align}
\frac{\partial f_{i}}{\partial p_{j}} & = \frac{0 - e^{p_{i}} e^{p_{j}}}{\left(\sum_{k=1}^{N}e^{p_{k}}\right)^{2}} \\
& = -\frac{e^{p_{i}}}{\sum_{k=1}^{N}e^{p_{k}}} \frac{e^{p_{j}}}{\sum_{k=1}^{N}e^{p_{k}}} \\[2ex]
& = -f_{i} f_{j}
\end{align}
$$

And thus,

$$
\frac{\partial f_{i}}{\partial p_{j}} =
\begin{cases}
f_{i}\left(1-f_{j}\right) & \text{$i$ = $j$} \\[2ex]
-f_{i} f_{j} & \text{$i$ $\neq$ $j$}
\end{cases}
$$

<a name='network'></a>

## Network Architecture
Let there be \\(l\\) layers in the network. Softmax Layer involves application of softmax activation to the last fully connected layer. Let the last fully connected layer be parametarized by \\(\left(W, b\right)\\) i.e, the weight matrix and bias vector. Further let \\(W \in \mathbb{R}^{NT}\\) and the input \\(X \in \mathbb{R}^{N}\\). Clearly this classification problem spans over \\(T\\) classes.

$$
W =
\begin{pmatrix}
w_{11} & w_{12} & \cdots & w_{1T} \\
w_{21} & w_{22} & \cdots & w_{2T} \\
\vdots & \vdots & \ddots & \vdots \\
w_{N1} & w_{N2} & \cdots & w_{NT} \\
\end{pmatrix}
\quad and \quad
X =
\begin{pmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{N} \\
\end{pmatrix}
$$

Representing \\(p\\) as "logits" and \\(f\\) the softmax function:

$$
\begin{align}
p & = W^{T}X \\
f & = f(p)
\end{align}
$$

We now have $$[f_{1}, f_{2}, ..., f_{T}]$$, which indicate the probability of belonging to that specific class. Comparing these values to the corresponding ground truth values $$y$$, which is a one hot vector we can define the cross entropy loss function as follows.

<a name='cross_entropy'></a>

## Cross Entropy Loss
Considering just one training example, the loss \\(L\\) is given by:

$$
L\left(y, f\right) = \sum_{i=1}^{T} -y_{i}\log\left(f_{i}\right)
$$

\\(y\\) is one hot vector representing the ground truth, \\(f\\) is the corresponding predictions which are obtained after applying softmax activation.

In order to update the weights during backpropagation, we need to compute the gradients

$$
\frac{\partial L}{\partial W_{kj}} \enspace k \in [1, N], j \in [1, T]
$$

Simple application of the chain rule gives us:

$$
\frac{\partial L}{\partial W_{kj}} = \frac{\partial L}{\partial f_{i}} \frac{\partial f_{i}}{\partial p_{j}} \frac{\partial p_{j}}{\partial W_{kj}} \quad \forall i,j \in [1, T], k \in [1, N]
$$

Computing the individual gradients gives us the following:

In the one hot vector $$y$$, let the index of the correct class be $$z$$ (i.e, the index at which the value is $$1$$). The loss can now be simplified to,

$$
L = -log\left(f_{z}\right)
\quad and \quad
\frac{\partial L}{\partial f_{i}} =
\begin{cases}
-\frac{1}{f_{z}} & \text{$i$ = $z$} \\[2ex]
0 & \text{$i$ $\neq$ $z$}
\end{cases}
$$

From above we already have:

$$
\frac{\partial f_{i}}{\partial p_{j}} =
\begin{cases}
f_{i}\left(1-f_{j}\right) & \text{$i$ = $j$} \\[2ex]
-f_{i} f_{j} & \text{$i$ $\neq$ $j$}
\end{cases}
$$

We have:

$$
\begin{align}
p_{1} & = w_{11}x_{1} + w_{21}x_{2} + \cdots + w_{N1}x_{N} \\
p_{2} & = w_{12}x_{1} + w_{22}x_{2} + \cdots + w_{N2}x_{N} \\
\vdots \\
p_{T} & = w_{1T}x_{1} + w_{2T}x_{2} + \cdots + w_{NT}x_{N} \\
\end{align}
$$

And thus,

$$
\frac{\partial p_{j}}{\partial W_{kj}} = x_{k} \quad \forall k \in [1, N], j \in [1, T]
$$

Putting it all together, since only the \\(z\\)th element in \\(y\\) is non zero:

$$
\begin{align}
\frac{\partial L}{\partial W_{kj}} & =
  \begin{cases}
    -\frac{1}{f_{z}} f_{z} \left(1 - f_{j}\right) x_{k} & \text{$z$ = $j$} \\[2ex]
    \frac{1}{f_{z}} f_{z} f_{j} x_{k} & \text{$z$ $\neq$ $j$} \\[2ex]
  \end{cases} \\[2ex]
& =
  \begin{cases}
    \left(f_{j} - 1\right) x_{k} & \text{$z$ = $j$} \\[2ex]
    f_{j} x_{k} & \text{$z$ $\neq$ $j$} \\[2ex]
  \end{cases} \\[2ex]
\end{align}
$$

<br><br>
<div id="disqus_thread"></div>
<script>
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://kvmanohar22-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
